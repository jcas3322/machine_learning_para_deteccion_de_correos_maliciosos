{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9328baa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import ipaddress\n",
    "from urllib.parse import urlparse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='sklearn')\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# === RUTAS / ARCHIVOS ===\n",
    "rutaBase = '/003_xgboost_phishing_count_vector_9000/'  # carpeta que contiene tus pkl\n",
    "input_csv = 'correos_normalizados.csv'                              # CSV de entrada con columnas: subject, body\n",
    "output_pred_csv = 'predicciones.csv'                   # CSV con resultados (subject, body, label, probabilidad)\n",
    "output_time_csv = 'tiempos.csv'                        # CSV con tiempos agregados (1 fila)\n",
    "\n",
    "# === NOMBRES DE COLUMNAS EN TU CSV ===\n",
    "SUBJECT_COL = 'subject'\n",
    "BODY_COL = 'body'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9506e072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modelos cargados correctamente.\n"
     ]
    }
   ],
   "source": [
    "def leer_modelos():\n",
    "    try:\n",
    "        model = joblib.load('model.pkl')\n",
    "        vectorizer = joblib.load('vectorizer.pkl')\n",
    "        scaler = joblib.load('scaler.pkl')\n",
    "        return model, vectorizer, scaler\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Fallo al cargar artefactos: {e}\")\n",
    "\n",
    "model, vectorizer, scaler = leer_modelos()\n",
    "print(\"✅ Modelos cargados correctamente.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a8a7595",
   "metadata": {},
   "outputs": [],
   "source": [
    "PHISHING_KEYWORDS = {\n",
    "    'english': {\n",
    "        'urgency': ['urgent', 'expire', 'suspend', 'immediate', 'limited time', 'act now', \n",
    "                    'hurry', 'deadline', 'critical', 'important notice'],\n",
    "        'action': ['click here', 'verify', 'confirm', 'update', 'validate', 'secure',\n",
    "                   'restore', 'unlock', 'activate', 'claim'],\n",
    "        'money': ['account', 'payment', 'billing', 'credit', 'debit', 'bank', 'paypal',\n",
    "                  'refund', 'prize', 'winner', 'lottery', 'tax'],\n",
    "        'threat': ['suspended', 'blocked', 'restricted', 'locked', 'illegal', 'unauthorized',\n",
    "                   'breach', 'compromised', 'violation', 'terminate']\n",
    "    },\n",
    "    'spanish': {\n",
    "        'urgency': ['urgente', 'expira', 'suspender', 'inmediato', 'tiempo limitado', \n",
    "                    'actúa ahora', 'apresúrate', 'fecha límite', 'crítico', 'aviso importante'],\n",
    "        'action': ['haga clic aquí', 'verificar', 'confirmar', 'actualizar', 'validar',\n",
    "                   'asegurar', 'restaurar', 'desbloquear', 'activar', 'reclamar'],\n",
    "        'money': ['cuenta', 'pago', 'facturación', 'crédito', 'débito', 'banco', 'paypal',\n",
    "                  'reembolso', 'premio', 'ganador', 'lotería', 'impuesto'],\n",
    "        'threat': ['suspendida', 'bloqueada', 'restringida', 'bloqueado', 'ilegal',\n",
    "                   'no autorizado', 'violación', 'comprometido', 'infracción', 'terminar']\n",
    "    }\n",
    "}\n",
    "\n",
    "def detect_language(text: str) -> str:\n",
    "    spanish_words = {'el', 'la', 'de', 'que', 'y', 'en', 'un', 'para'}\n",
    "    english_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that'}\n",
    "    words = (text or \"\").lower().split()[:50]\n",
    "    if sum(w in spanish_words for w in words) > sum(w in english_words for w in words):\n",
    "        return 'spanish'\n",
    "    return 'english'\n",
    "\n",
    "def phishing_preprocessing(text: str) -> str:\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "\n",
    "    # Preservar URLs/emails temporalmente\n",
    "    url_pattern = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    urls = re.findall(url_pattern, text)\n",
    "    for i, url in enumerate(urls):\n",
    "        text = text.replace(url, f\" URL{i} \")\n",
    "\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text)\n",
    "    for i, em in enumerate(emails):\n",
    "        text = text.replace(em, f\" EMAIL{i} \")\n",
    "\n",
    "    text = text.lower()\n",
    "\n",
    "    # Restaurar\n",
    "    for i, url in enumerate(urls):\n",
    "        text = text.replace(f\"url{i}\", url.lower())\n",
    "    for i, em in enumerate(emails):\n",
    "        text = text.replace(f\"email{i}\", em.lower())\n",
    "\n",
    "    text = re.sub(r'_{3,}', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def extract_url_features(text: str) -> dict:\n",
    "    feats = {\n",
    "        'url_count': 0,\n",
    "        'shortened_url': 0,\n",
    "        'has_ip': 0,\n",
    "        'suspicious_domain': 0,  # placeholder simple\n",
    "        'long_url': 0,\n",
    "        'has_at_symbol': 0,\n",
    "        'multiple_subdomains': 0,\n",
    "        'https_count': 0,\n",
    "        'http_count': 0\n",
    "    }\n",
    "    u_pat = r'https?://(?:[-\\w.]|(?:%[\\da-fA-F]{2}))+'\n",
    "    urls = re.findall(u_pat, (text or \"\").lower())\n",
    "    feats['url_count'] = len(urls)\n",
    "\n",
    "    shorteners = ['bit.ly', 'tinyurl', 'goo.gl', 'ow.ly', 'short.link', 't.co']\n",
    "    suspicious_tlds = {'.ru', '.tk', '.cn'}  # heurística opcional\n",
    "\n",
    "    for url in urls:\n",
    "        if any(s in url for s in shorteners):\n",
    "            feats['shortened_url'] += 1\n",
    "        try:\n",
    "            parsed = urlparse(url)\n",
    "            if parsed.hostname:\n",
    "                try:\n",
    "                    ipaddress.ip_address(parsed.hostname)\n",
    "                    feats['has_ip'] += 1\n",
    "                except ValueError:\n",
    "                    pass\n",
    "                # Heurística muy básica: TLD sospechoso o muchos guiones\n",
    "                host = parsed.hostname\n",
    "                if any(host.endswith(tld) for tld in suspicious_tlds) or host.count('-') >= 3:\n",
    "                    feats['suspicious_domain'] += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "        if len(url) > 75:\n",
    "            feats['long_url'] += 1\n",
    "        if '@' in url:\n",
    "            feats['has_at_symbol'] += 1\n",
    "        if url.count('.') > 3:\n",
    "            feats['multiple_subdomains'] += 1\n",
    "        if url.startswith('https'):\n",
    "            feats['https_count'] += 1\n",
    "        elif url.startswith('http:'):\n",
    "            feats['http_count'] += 1\n",
    "    return feats\n",
    "\n",
    "def extract_phishing_features(text: str, language='english') -> dict:\n",
    "    feats = {}\n",
    "    tlower = (text or \"\").lower()\n",
    "    kw = PHISHING_KEYWORDS.get(language, PHISHING_KEYWORDS['english'])\n",
    "    for category, words in kw.items():\n",
    "        feats[f'{category}_words'] = sum(w in tlower for w in words)\n",
    "    feats['exclamation_count'] = tlower.count('!')\n",
    "    feats['question_count'] = tlower.count('?')\n",
    "    feats['uppercase_words'] = len(re.findall(r'\\b[A-Z]{2,}\\b', text or \"\"))\n",
    "    feats['special_chars'] = len(re.findall(r'[!@#$%^&*(),.?\":{}|<>]', text or \"\"))\n",
    "\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "    emails = re.findall(email_pattern, text or \"\")\n",
    "    feats['email_count'] = len(emails)\n",
    "\n",
    "    legitimate_domains = [\n",
    "        'paypal.com','amazon.com','google.com','microsoft.com','apple.com',\n",
    "        'facebook.com','chase.com','netflix.com'\n",
    "    ]\n",
    "    suspicious_email = 0\n",
    "    for em in emails:\n",
    "        domain = em.split('@')[-1].lower()\n",
    "        for legit in legitimate_domains:\n",
    "            if domain != legit and (\n",
    "                domain.replace('0','o') == legit or\n",
    "                domain.replace('1','l') == legit or\n",
    "                legit.split('.')[0] in domain and domain != legit\n",
    "            ):\n",
    "                suspicious_email += 1\n",
    "                break\n",
    "    feats['suspicious_email'] = suspicious_email\n",
    "    return feats\n",
    "\n",
    "FEATURE_ORDER = [\n",
    "    'url_count','shortened_url','has_ip','suspicious_domain','long_url','has_at_symbol',\n",
    "    'multiple_subdomains','https_count','http_count','urgency_words','action_words',\n",
    "    'money_words','threat_words','exclamation_count','question_count','uppercase_words',\n",
    "    'special_chars','email_count','suspicious_email'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f028ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparar_batch_features(texts: list[str]) -> tuple:\n",
    "    \"\"\"\n",
    "    Devuelve:\n",
    "      - text_matrix: matriz densa/CSR de vectorizer.transform\n",
    "      - extra_scaled: matriz numpy con features extras escaladas en el mismo orden que FEATURE_ORDER\n",
    "    \"\"\"\n",
    "    # Preproceso para vectorizer\n",
    "    preprocessed = [phishing_preprocessing(t) for t in texts]\n",
    "    text_matrix = vectorizer.transform(preprocessed)\n",
    "\n",
    "    # Extras (no preprocesados, usan el texto \"original\")\n",
    "    lang_list = [detect_language(t or \"\") for t in texts]\n",
    "    extras = []\n",
    "    for t, lang in zip(texts, lang_list):\n",
    "        u = extract_url_features(t or \"\")\n",
    "        p = extract_phishing_features(t or \"\", language=lang)\n",
    "        allf = {**u, **p}\n",
    "        extras.append([allf.get(name, 0) for name in FEATURE_ORDER])\n",
    "\n",
    "    extras_df = pd.DataFrame(extras, columns=FEATURE_ORDER)\n",
    "    extra_scaled = scaler.transform(extras_df)\n",
    "    return text_matrix, extra_scaled\n",
    "\n",
    "def predecir_batch(texts: list[str]) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Retorna:\n",
    "      - labels: array de 0/1\n",
    "      - probs: probabilidad clase 1 (phishing) como float\n",
    "    \"\"\"\n",
    "    X_text, X_extra = preparar_batch_features(texts)\n",
    "    if hasattr(X_text, 'toarray'):\n",
    "        X_comb = np.hstack([X_text.toarray(), X_extra])\n",
    "    else:\n",
    "        X_comb = np.hstack([X_text, X_extra])\n",
    "    labels = model.predict(X_comb)\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        probs = model.predict_proba(X_comb)[:, 1]\n",
    "    else:\n",
    "        # Si no hay predict_proba, usar decisión calibrada simple (fallback)\n",
    "        if hasattr(model, \"decision_function\"):\n",
    "            df = model.decision_function(X_comb)\n",
    "            # escalado sigmoid\n",
    "            probs = 1 / (1 + np.exp(-df))\n",
    "        else:\n",
    "            probs = labels.astype(float)\n",
    "    return labels, probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da77c0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Archivo de predicciones guardado en: predicciones.csv\n",
      "⏱️  Archivo de tiempos guardado en: tiempos.csv\n",
      "→ Segundos totales: 2.7532  |  Promedio por correo: 0.001335\n"
     ]
    }
   ],
   "source": [
    "# === Cargar el CSV de entrada ===\n",
    "df = pd.read_csv(input_csv)\n",
    "if SUBJECT_COL not in df.columns or BODY_COL not in df.columns:\n",
    "    raise ValueError(f\"El CSV debe contener las columnas '{SUBJECT_COL}' y '{BODY_COL}'.\")\n",
    "\n",
    "# Ensamblar texto final a evaluar\n",
    "subjects = df[SUBJECT_COL].fillna(\"\").astype(str)\n",
    "bodies = df[BODY_COL].fillna(\"\").astype(str)\n",
    "texts = (subjects + \" \" + bodies).tolist()\n",
    "\n",
    "# === Medir tiempo total de predicciones ===\n",
    "t0 = time.perf_counter()\n",
    "labels, probs = predecir_batch(texts)\n",
    "t1 = time.perf_counter()\n",
    "\n",
    "# === Armar salida de predicciones ===\n",
    "out = df[[SUBJECT_COL, BODY_COL]].copy()\n",
    "out['label'] = (labels.astype(int))           # 1=malicioso, 0=legítimo\n",
    "out['probabilidad'] = probs\n",
    "out.to_csv(output_pred_csv, index=False, encoding='utf-8')\n",
    "print(f\"✅ Archivo de predicciones guardado en: {output_pred_csv}\")\n",
    "\n",
    "# === CSV de tiempos (agregado) ===\n",
    "n = len(texts)\n",
    "elapsed = t1 - t0\n",
    "avg = (elapsed / n) if n else 0.0\n",
    "time_row = {\n",
    "    'fecha': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'n_correos': n,\n",
    "    'segundos_totales': round(elapsed, 6),\n",
    "    'segundos_promedio_por_correo': round(avg, 6),\n",
    "    'modelo_path': rutaBase\n",
    "}\n",
    "pd.DataFrame([time_row]).to_csv(output_time_csv, index=False, encoding='utf-8')\n",
    "print(f\"⏱️  Archivo de tiempos guardado en: {output_time_csv}\")\n",
    "print(f\"→ Segundos totales: {elapsed:.4f}  |  Promedio por correo: {avg:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
