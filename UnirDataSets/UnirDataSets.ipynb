{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8291b10f-3c17-43e5-b884-14db5dd1a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ INICIANDO PROCESO DE UNI√ìN DE DATASETS\n",
      "==================================================\n",
      "‚úÖ Pandas disponible\n",
      "=== PROCESANDO ARCHIVOS ===\n",
      "\n",
      "üìÅ Procesando: Nigerian_Fraud_es.csv\n",
      "   Filas originales: 3332\n",
      "   Columnas originales: ['sender', 'receiver', 'date', 'subject', 'body', 'urls', 'label']\n",
      "   ‚úÖ Procesado: 3194 filas mantenidas\n",
      "\n",
      "üìÅ Procesando: Ling_es.csv\n",
      "   Filas originales: 2859\n",
      "   Columnas originales: ['subject', 'body', 'label']\n",
      "   ‚úÖ Procesado: 2790 filas mantenidas\n",
      "\n",
      "üìÅ Procesando: CEAS_08_es.csv\n",
      "   Filas originales: 39154\n",
      "   Columnas originales: ['sender', 'receiver', 'date', 'subject', 'body', 'label', 'urls']\n",
      "   ‚úÖ Procesado: 35429 filas mantenidas\n",
      "\n",
      "=== UNIENDO DATASETS ===\n",
      "üìä Dataset final:\n",
      "   Total de filas: 41413\n",
      "   Columnas: ['subject', 'body', 'label', 'source_file']\n",
      "\n",
      "üìà Distribuci√≥n por archivo:\n",
      "   CEAS_08_es: 35429 filas\n",
      "   Nigerian_Fraud_es: 3194 filas\n",
      "   Ling_es: 2790 filas\n",
      "\n",
      "üè∑Ô∏è  Distribuci√≥n por etiqueta:\n",
      "   1: 21886 filas\n",
      "   0: 19527 filas\n",
      "\n",
      "‚úÖ Archivo guardado como: dataset_unificado.csv\n",
      "\n",
      "üìã Muestra de los primeros 3 registros:\n",
      "                                        subject                                               body label       source_file\n",
      "ASISTENCIA A LAS EMPRESAS URGENTES Y ASOCIACI√ìN DE:MR. James NGOLA. TEL. CONFIDENCIAL: 233-27-5...     1 Nigerian_Fraud_es\n",
      "              ASISTENCIA URGENTE / RELACI√ìN (P) Querido amigo, soy el Sr. Ben Suleman un Office...     1 Nigerian_Fraud_es\n",
      "                              Buenos d√≠as a ti. DE SU MAJESTAD ROYAL (HRM) R√öBRICA CROWN DEL RE...     1 Nigerian_Fraud_es\n",
      "\n",
      "üéâ ¬°Proceso completado exitosamente!\n",
      "   El archivo 'dataset_unificado.csv' est√° listo para usar\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def merge_datasets():\n",
    "    \"\"\"\n",
    "    Une tres datasets CSV manteniendo solo las columnas comunes: subject, body, label\n",
    "    \"\"\"\n",
    "    \n",
    "    # Nombres de los archivos\n",
    "    files = ['Nigerian_Fraud_es.csv', 'Ling_es.csv', 'CEAS_08_es.csv']\n",
    "    \n",
    "    # Columnas que queremos mantener\n",
    "    target_columns = ['subject', 'body', 'label']\n",
    "    \n",
    "    # Lista para almacenar los dataframes procesados\n",
    "    processed_datasets = []\n",
    "    \n",
    "    print(\"=== PROCESANDO ARCHIVOS ===\")\n",
    "    \n",
    "    for file in files:\n",
    "        try:\n",
    "            # Verificar que el archivo existe\n",
    "            if not os.path.exists(file):\n",
    "                print(f\"‚ùå Archivo no encontrado: {file}\")\n",
    "                continue\n",
    "            \n",
    "            # Leer el CSV\n",
    "            print(f\"\\nüìÅ Procesando: {file}\")\n",
    "            df = pd.read_csv(file, encoding='utf-8')\n",
    "            \n",
    "            # Mostrar informaci√≥n del archivo original\n",
    "            print(f\"   Filas originales: {len(df)}\")\n",
    "            print(f\"   Columnas originales: {list(df.columns)}\")\n",
    "            \n",
    "            # Verificar que las columnas objetivo existen\n",
    "            missing_columns = [col for col in target_columns if col not in df.columns]\n",
    "            if missing_columns:\n",
    "                print(f\"   ‚ö†Ô∏è  Columnas faltantes: {missing_columns}\")\n",
    "                \n",
    "                # Intentar mapear nombres similares\n",
    "                column_mapping = {}\n",
    "                for col in missing_columns:\n",
    "                    similar_cols = [c for c in df.columns if col.lower() in c.lower() or c.lower() in col.lower()]\n",
    "                    if similar_cols:\n",
    "                        print(f\"   üîç Columnas similares encontradas para '{col}': {similar_cols}\")\n",
    "                        # Aqu√≠ podr√≠as agregar l√≥gica para mapear autom√°ticamente\n",
    "                        # Por ahora, mostramos las opciones\n",
    "                \n",
    "                # Continuar con las columnas que s√≠ existen\n",
    "                available_columns = [col for col in target_columns if col in df.columns]\n",
    "                if not available_columns:\n",
    "                    print(f\"   ‚ùå No se encontraron columnas objetivo en {file}\")\n",
    "                    continue\n",
    "                \n",
    "                df_filtered = df[available_columns].copy()\n",
    "                \n",
    "                # Agregar columnas faltantes con valores vac√≠os\n",
    "                for col in missing_columns:\n",
    "                    df_filtered[col] = ''\n",
    "                    \n",
    "            else:\n",
    "                # Filtrar solo las columnas que necesitamos\n",
    "                df_filtered = df[target_columns].copy()\n",
    "            \n",
    "            # Agregar columna de origen\n",
    "            df_filtered['source_file'] = file.replace('.csv', '')\n",
    "            \n",
    "            # Limpiar datos\n",
    "            df_filtered = df_filtered.dropna(subset=['subject', 'body'])  # Eliminar filas sin subject o body\n",
    "            df_filtered = df_filtered.drop_duplicates()  # Eliminar duplicados\n",
    "            \n",
    "            # Limpiar espacios en blanco\n",
    "            for col in ['subject', 'body', 'label']:\n",
    "                if col in df_filtered.columns:\n",
    "                    df_filtered[col] = df_filtered[col].astype(str).str.strip()\n",
    "            \n",
    "            processed_datasets.append(df_filtered)\n",
    "            print(f\"   ‚úÖ Procesado: {len(df_filtered)} filas mantenidas\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error procesando {file}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Verificar que tenemos al menos un dataset procesado\n",
    "    if not processed_datasets:\n",
    "        print(\"\\n‚ùå No se pudo procesar ning√∫n archivo\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== UNIENDO DATASETS ===\")\n",
    "    \n",
    "    # Unir todos los datasets\n",
    "    merged_df = pd.concat(processed_datasets, ignore_index=True)\n",
    "    \n",
    "    # Reordenar columnas\n",
    "    column_order = ['subject', 'body', 'label', 'source_file']\n",
    "    merged_df = merged_df[column_order]\n",
    "    \n",
    "    # Mostrar estad√≠sticas finales\n",
    "    print(f\"üìä Dataset final:\")\n",
    "    print(f\"   Total de filas: {len(merged_df)}\")\n",
    "    print(f\"   Columnas: {list(merged_df.columns)}\")\n",
    "    \n",
    "    # Mostrar distribuci√≥n por archivo fuente\n",
    "    print(f\"\\nüìà Distribuci√≥n por archivo:\")\n",
    "    source_counts = merged_df['source_file'].value_counts()\n",
    "    for source, count in source_counts.items():\n",
    "        print(f\"   {source}: {count} filas\")\n",
    "    \n",
    "    # Mostrar distribuci√≥n por etiqueta (si existe)\n",
    "    if 'label' in merged_df.columns and not merged_df['label'].isna().all():\n",
    "        print(f\"\\nüè∑Ô∏è  Distribuci√≥n por etiqueta:\")\n",
    "        label_counts = merged_df['label'].value_counts()\n",
    "        for label, count in label_counts.items():\n",
    "            print(f\"   {label}: {count} filas\")\n",
    "    \n",
    "    # Guardar el archivo unificado\n",
    "    output_file = 'dataset_unificado.csv'\n",
    "    merged_df.to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"\\n‚úÖ Archivo guardado como: {output_file}\")\n",
    "    \n",
    "    # Mostrar muestra de los datos\n",
    "    print(f\"\\nüìã Muestra de los primeros 3 registros:\")\n",
    "    print(merged_df.head(3).to_string(index=False, max_colwidth=50))\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Funci√≥n principal\n",
    "    \"\"\"\n",
    "    print(\"üöÄ INICIANDO PROCESO DE UNI√ìN DE DATASETS\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Verificar que pandas est√° instalado\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        print(\"‚úÖ Pandas disponible\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Error: pandas no est√° instalado\")\n",
    "        print(\"   Instala con: pip install pandas\")\n",
    "        return\n",
    "    \n",
    "    # Ejecutar el proceso\n",
    "    result = merge_datasets()\n",
    "    \n",
    "    if result is not None:\n",
    "        print(\"\\nüéâ ¬°Proceso completado exitosamente!\")\n",
    "        print(f\"   El archivo 'dataset_unificado.csv' est√° listo para usar\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå El proceso fall√≥\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b261f-109f-4ad1-968c-3352d7a9adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
